import requests
from bs4 import BeautifulSoup
import time

def fetch_and_parse_html(url):
    # 设置User-Agent头以模拟浏览器访问
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/{number}.3'}
    try:
        # 使用requests发送GET请求获取网页内容
        response = requests.get(url, headers=headers)
        # 检查请求是否成功
        response.raise_for_status()
        
        # 使用BeautifulSoup解析HTML文档
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup
    except requests.RequestException as e:
        # 如果请求失败，打印错误信息并返回None
        print(f"请求错误: {e}")
        return None

def extract_info(soup):
    # 初始化一个字典来存储提取的信息
    info = {}
    # 查找所有具有指定类名的div元素
    div_row = soup.find_all('div', {'class': 'row'})
    
    for row in div_row:
        # 查找所有的标题和统计数据标题
        info_titles = row.find_all('h4', {'class': 'info-title'})
        stats_titles = row.find_all('h6', {'class': 'stats-title'})

        # 遍历所有的标题和对应的统计数据
        for title, stat in zip(info_titles, stats_titles):
            # 清理文本数据
            text_title = title.text.strip().lower()
            text_stat = stat.text.strip()

            # 根据标题类型填充信息字典
            if 'edb-id:' in text_title:
                info['EDB-ID'] = text_stat if text_stat.isdigit() else None
            elif 'cve:' in text_title:
                info['CVE'] = text_stat if text_stat != 'n/a' else None
            elif 'author:' in text_title:
                info['作者'] = text_stat
            elif 'type:' in text_title:
                info['类型'] = text_stat
            elif 'platform:' in text_title:
                info['平台'] = text_stat
            elif 'date:' in text_title:
                info['日期'] = text_stat
        
    return info

def save_info_to_file(info, filename, number):
    try:
        # 尝试打开或创建文件用于写入
        with open(filename, 'w', encoding='utf-8') as file:
            # 写入提取的信息
            for key, value in info.items():
                file.write(f"{key}: {value}\n")
            # 添加原始链接信息
            raw_url = f"https://www.exploit-db.com/raw/{number}"
            file.write(f"Raw URL: {raw_url}\n")
        # 打印保存成功的消息
        print(f"已保存信息到 {filename}")
    except IOError as e:
        # 如果写入失败，打印错误信息
        print(f"无法写入文件 {filename}: {e}")

def main():
    # 设置基础URL模板
    base_url = "https://www.exploit-db.com/exploits/{number}"  
    # 循环遍历一系列编号
    for number in range(51934, 51658, -1):  
        # 构建完整的URL
        url = base_url.format(number=number)
        print(f"正在处理 {url}")
        # 获取并解析HTML
        soup = fetch_and_parse_html(url)
        if soup:
            # 提取所需信息
            info = extract_info(soup)
            # 设置输出文件名
            filename = f"{number}.txt"
            # 将信息保存到文件中
            save_info_to_file(info, filename, number)
        # 休眠一段时间，避免对服务器造成过大压力
        #time.sleep(0.5)

if __name__ == "__main__":
    # 当脚本直接运行时执行主函数
    main()